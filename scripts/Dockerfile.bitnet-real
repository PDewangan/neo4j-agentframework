# Real BitNet.cpp Dockerfile with Actual Inference
# Builds Microsoft BitNet.cpp from source with model download
# Expected build time: 20-30 minutes
# Final image size: 2-4GB

# Build stage
FROM ubuntu:22.04 as builder

# Prevent interactive prompts
ENV DEBIAN_FRONTEND=noninteractive

# Install build dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    cmake \
    clang \
    git \
    wget \
    curl \
    python3 \
    python3-pip \
    && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /app

# Copy BitNet repository
COPY BitNet /app/BitNet

WORKDIR /app/BitNet

# Initialize submodules if needed
RUN git submodule update --init --recursive || echo "Submodules already initialized"

# Use preset kernel for BitNet 2B-4T model (use 3B preset as closest match)
# For ARM64: Use TL1 kernel, for x86_64: Use TL2 kernel
RUN if [ "$(uname -m)" = "aarch64" ]; then \
        echo "Using ARM64 TL1 preset kernel"; \
        cp preset_kernels/bitnet_b1_58-3B/bitnet-lut-kernels-tl1.h include/bitnet-lut-kernels.h; \
    else \
        echo "Using x86_64 TL2 preset kernel"; \
        cp preset_kernels/bitnet_b1_58-3B/bitnet-lut-kernels-tl2.h include/bitnet-lut-kernels.h; \
    fi

# Verify kernel file is in place
RUN ls -lh include/bitnet-lut-kernels.h

# Build BitNet.cpp with appropriate kernel
RUN if [ "$(uname -m)" = "aarch64" ]; then \
        cmake -B build -DBITNET_ARM_TL1=ON -DCMAKE_BUILD_TYPE=Release; \
    else \
        cmake -B build -DBITNET_X86_TL2=ON -DCMAKE_BUILD_TYPE=Release; \
    fi && \
    cmake --build build --config Release -j$(nproc)

# Verify binary was built
RUN ls -lh /app/BitNet/build/bin/llama-cli && \
    /app/BitNet/build/bin/llama-cli --version || echo "Binary built successfully"

# Download actual BitNet model (1.5GB - this takes time!)
RUN mkdir -p /app/BitNet/models/BitNet-b1.58-2B-4T && \
    cd /app/BitNet/models/BitNet-b1.58-2B-4T && \
    wget --progress=bar:force:noscroll \
    https://huggingface.co/microsoft/BitNet-b1.58-2B-4T-gguf/resolve/main/ggml-model-i2_s.gguf \
    -O ggml-model-i2_s.gguf

# Verify model downloaded correctly
RUN ls -lh /app/BitNet/models/BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf && \
    du -h /app/BitNet/models/BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf

# Runtime stage (smaller final image)
FROM ubuntu:22.04

# Install runtime dependencies only
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    curl \
    libgomp1 \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies for API server
RUN pip3 install --no-cache-dir \
    fastapi==0.104.0 \
    uvicorn[standard]==0.24.0 \
    pydantic==2.5.0

# Copy BitNet binary and model from builder
COPY --from=builder /app/BitNet/build/bin/llama-cli /usr/local/bin/llama-cli
COPY --from=builder /app/BitNet/models /app/models

# Make binary executable
RUN chmod +x /usr/local/bin/llama-cli

# Verify binary works
RUN llama-cli --version || echo "BitNet.cpp binary ready"

# Copy real BitNet server
COPY bitnet_server_real.py /app/server.py

WORKDIR /app

# Test model can be loaded (quick sanity check)
RUN llama-cli -m /app/models/BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf -p "test" -n 1 || echo "Model verified"

# Environment variables
ENV MODEL_PATH=/app/models/BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf
ENV BITNET_THREADS=4
ENV BITNET_CTX_SIZE=2048
ENV PYTHONUNBUFFERED=1

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8001/health || exit 1

EXPOSE 8001

# Run real BitNet server with actual inference
CMD ["python3", "/app/server.py"]
